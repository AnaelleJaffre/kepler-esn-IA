{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f639b94a",
   "metadata": {},
   "source": [
    "# Hyperparameter Tuning for ESN on Kepler Time Series\n",
    "\n",
    "## 1. Problem Statement\n",
    "\n",
    "In our first experiment with a small Echo State Network (ESN) on the Kepler labelled dataset, the results showed:\n",
    "\n",
    "- **Accuracy:** 99%\n",
    "- **AUC:** 0.7\n",
    "- **Errors:** 5 out of 570  \n",
    "\n",
    "At first sight, this seems excellent. However, **all errors correspond to stars that actually have exoplanets**, which means the model **never detected any exoplanet**.  \n",
    "\n",
    "**Problem:** The ESN is biased toward the majority class (no exoplanet) due to the extreme class imbalance in the dataset. The goal of this notebook is to **fine-tune the ESN hyperparameters** to improve detection of exoplanets.\n",
    "\n",
    "## 2. Initial Setup\n",
    "\n",
    "We start with the base parameters used previously:\n",
    "\n",
    "```python\n",
    "units = 20          # number of neurons in the reservoir\n",
    "leak_rate = 0.3     # leak rate (memory retention of neurons)\n",
    "input_scaling = 0.5 # influence of input on the reservoir\n",
    "spectral_radius = 0.9 # strength of reservoir dynamics\n",
    "ridge = 1e-6        # regularization\n",
    "```\n",
    "\n",
    "## 3. Parameters to Optimize\n",
    "\n",
    "Based on the tutorial and our observations, we can focus on the following hyperparameters:\n",
    "\n",
    "Parameter\tDescription\tRange / Notes\n",
    "units\tNumber of neurons in the reservoir\t20 → 200\n",
    "leak_rate\tMemory of the reservoir\t0.1 → 1\n",
    "input_scaling\tInfluence of input on reservoir\t0.1 → 1\n",
    "spectral_radius\tStrength of dynamics\t0.5 → 1.5\n",
    "ridge\tRegularization (ridge)\t1e-8 → 1e-2\n",
    "\n",
    "**Goal:** increase the network's ability to detect the minority class (exoplanets) without destabilizing the reservoir.\n",
    "\n",
    "## 4. Strategy\n",
    "\n",
    "1. Fix the random seed to ensure reproducibility.\n",
    "\n",
    "2. Normalize each series individually (mean 0, std 1).\n",
    "\n",
    "3. Define a grid or random search over the above hyperparameters.\n",
    "\n",
    "4. For each configuration:\n",
    "\n",
    "    - Train the ESN\n",
    "\n",
    "    - Compute accuracy, AUC and get the number of errors\n",
    "\n",
    "5. Select hyperparameters that maximize AUC (more informative than accuracy due to imbalance) and minimize the number of errors.\n",
    "\n",
    "## 5. Load Dataset and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0c31ae34",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Load your train/test datasets\n",
    "train_df = pd.read_csv(\"../data/exoTrain.csv\")\n",
    "test_df = pd.read_csv(\"../data/exoTest.csv\")\n",
    "\n",
    "# Extract labels\n",
    "y_train_np = np.asarray(train_df.iloc[:,0], dtype=int) - 1  # 0 = no exoplanet, 1 = exoplanet\n",
    "y_test_np  = np.asarray(test_df.iloc[:,0], dtype=int) - 1\n",
    "\n",
    "# Extract time series data\n",
    "X_train = train_df.iloc[:, 1:].values[..., np.newaxis]\n",
    "X_test  = test_df.iloc[:, 1:].values[..., np.newaxis]\n",
    "\n",
    "# Convert to list of sequences\n",
    "X_train_seq = [x for x in X_train]\n",
    "X_test_seq  = [x for x in X_test]\n",
    "\n",
    "# Normalize per series\n",
    "X_train_seq = [(x - x.mean()) / (x.std() + 1e-8) for x in X_train_seq]\n",
    "X_test_seq  = [(x - x.mean()) / (x.std() + 1e-8) for x in X_test_seq]\n",
    "\n",
    "# Create sequential labels for ESN\n",
    "y_train_seq = [np.full((x.shape[0], 1), y) for x, y in zip(X_train_seq, y_train_np)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "647c9b9d",
   "metadata": {},
   "source": [
    "## 6. Example: Base ESN Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78a6f785",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 0.9912280701754386,\n",
      "Area Under the ROC Curve = 0.7139823008849557, Confusion matrice: [[565   0]\n",
      " [  5   0]]\n",
      "Number of errors: 5 on 570\n"
     ]
    }
   ],
   "source": [
    "from reservoirpy.nodes import Reservoir, Ridge\n",
    "\n",
    "# Base ESN\n",
    "reservoir = Reservoir(\n",
    "    units=20,\n",
    "    lr=0.3,\n",
    "    input_scaling=0.5,\n",
    "    sr=0.9\n",
    ")\n",
    "\n",
    "readout = Ridge(ridge=1e-6)\n",
    "\n",
    "esn = reservoir >> readout\n",
    "\n",
    "# Train\n",
    "esn.fit(X_train_seq, y_train_seq)\n",
    "\n",
    "# Predict\n",
    "y_pred_seq = esn.run(X_test_seq)\n",
    "y_pred = np.array([yp.mean() for yp in y_pred_seq])\n",
    "y_pred_label = (y_pred > 0.5).astype(int)\n",
    "\n",
    "# Evaluate\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, confusion_matrix\n",
    "\n",
    "accuracy = accuracy_score(y_test_np, y_pred_label)\n",
    "auc = roc_auc_score(y_test_np, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4f5cf8a",
   "metadata": {},
   "source": [
    "#### Performance analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cc94fc6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 99.12%,\n",
      "Area Under the ROC Curve = 0.71\n",
      "Number of errors: 5 on 570\n"
     ]
    }
   ],
   "source": [
    "print(f\"Accuracy = {round(accuracy*100, 2)}%,\\nArea Under the ROC Curve = {round(auc, 2)}\")\n",
    "\n",
    "# Errors indices\n",
    "errors = np.where(y_test_np != y_pred_label)[0]\n",
    "print(f\"Number of errors: {len(errors)} on {len(y_test_np)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c7bb6c1",
   "metadata": {},
   "source": [
    "\n",
    "Here are the results obtained in the first experiment with base parameters. They look great, but the network **fails** to recognise stars with an exoplanet.\n",
    "\n",
    "## 7. Hyperparameter Fine-tuning\n",
    "\n",
    "Now that the ESN is all set up, we can start the experiment.\n",
    "\n",
    "### Methodology \n",
    "\n",
    "The methodology we propose here is inspired by the paper *[Which Hype for my New Task? Hints and Random Search for Echo State Networks Hyperparameters](https://inria.hal.science/hal-03203318v2/document)*, published in 2021 by Xavier Hinaut and Nathan Trouvain. In this paper, the authors suggest that random search is more efficient than grid search for ESN hyperparameters optimization.\n",
    "\n",
    "Thus, in the following steps, we will finetune hyperparameters thanks to a random search, instead of a grid search.\n",
    "\n",
    "### Why random search?\n",
    "\n",
    "The paper written by Xavier Hinaut and Nathan Trouvain explains that in a grid search, evaluations can be wasted in **low-impact regions**. Indeed, in reservoir computing, overall performance on hyperparameters is rather **non-linear and irregular**. There are a lot of chance than gird-search only sees low-impact regions because of these non-lineariries.\n",
    "\n",
    "The advantage of random search is that it **samples uniformly** across the full range of each parameter. Hence, it covers more independent combinations and is therefore more likely to hit effective configurations with **fewer trials**. Because only a few hyperparameters strongly influence performance, random search spreads the search more efficiently in order to explore these influential directions. This approach reduces the redundancy inherent in grids and achieves better performance for equivalent search conditions.\n",
    "\n",
    "### Applying random search to optimise hyperparameters\n",
    "\n",
    "Here is a script that proposes a simple random search for optimisation, applied to our model and dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ed256800",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from reservoirpy.nodes import Reservoir\n",
    "\n",
    "# -----------------------\n",
    "# Random search parameters\n",
    "# -----------------------\n",
    "np.random.seed(99)  # reproducibility\n",
    "n_configs = 6       # number of random configurations to try\n",
    "\n",
    "units_range = [5, 10, 15, 20] # Not pushing my computer too hard...\n",
    "spectral_radius_range = [0.1, 0.9, 1.25, 10.0]\n",
    "input_scaling_range = [0.1, 0.5, 1.0]\n",
    "leak_rate_range = [0.1, 0.3, 0.5, 0.9]\n",
    "rc_connectivity_range = [0.05, 0.1, 0.5]\n",
    "input_connectivity_range = [0.1, 0.5, 1.0]\n",
    "\n",
    "# -----------------------\n",
    "# Run random search\n",
    "# -----------------------\n",
    "states = []\n",
    "params_list = []\n",
    "\n",
    "# Optionally take only first 500 timesteps to reduce computation\n",
    "X_subset_seq = [x[:500] for x in X_train_seq]\n",
    "\n",
    "for i in range(n_configs):\n",
    "    UNITS = np.random.choice(units_range)\n",
    "    SPECTRAL_RADIUS = np.random.choice(spectral_radius_range)\n",
    "    INPUT_SCALING = np.random.choice(input_scaling_range)\n",
    "    LEAK_RATE = np.random.choice(leak_rate_range)\n",
    "    RC_CONNECTIVITY = np.random.choice(rc_connectivity_range)\n",
    "    INPUT_CONNECTIVITY = np.random.choice(input_connectivity_range)\n",
    "    SEED = np.random.randint(0, 10000)\n",
    "\n",
    "    reservoir = Reservoir(\n",
    "        units=UNITS,\n",
    "        sr=SPECTRAL_RADIUS,\n",
    "        input_scaling=INPUT_SCALING,\n",
    "        lr=LEAK_RATE,\n",
    "        rc_connectivity=RC_CONNECTIVITY,\n",
    "        input_connectivity=INPUT_CONNECTIVITY,\n",
    "        seed=SEED\n",
    "    )\n",
    "\n",
    "    # Run reservoir on subset\n",
    "    s = reservoir.run(X_subset_seq)\n",
    "    states.append(s)\n",
    "\n",
    "    # Save configuration for reference\n",
    "    params_list.append({\n",
    "        \"units\": UNITS,\n",
    "        \"spectral_radius\": SPECTRAL_RADIUS,\n",
    "        \"input_scaling\": INPUT_SCALING,\n",
    "        \"leak_rate\": LEAK_RATE,\n",
    "        \"rc_connectivity\": RC_CONNECTIVITY,\n",
    "        \"input_connectivity\": INPUT_CONNECTIVITY,\n",
    "        \"seed\": SEED\n",
    "    })\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec427e90",
   "metadata": {},
   "source": [
    "### Visualisation\n",
    "\n",
    "#### Spectral Radius"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9f410fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "UNITS_SHOWN = 10\n",
    "\n",
    "plt.figure(figsize=(15, 4 * n_configs))\n",
    "for i, s_list in enumerate(states):\n",
    "    s = np.concatenate(s_list, axis=0) # Concatenate all sequences along the time axis\n",
    "    plt.subplot(n_configs, 1, i + 1)\n",
    "    plt.plot(s[:, :UNITS_SHOWN], alpha=0.6)\n",
    "    plt.ylabel(f\"Config {i+1}: sr={params_list[i]['spectral_radius']}, units={params_list[i]['units']}\")\n",
    "plt.xlabel(f\"Activations ({UNITS_SHOWN} neurons)\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kepler-esn-env (3.13.2)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
